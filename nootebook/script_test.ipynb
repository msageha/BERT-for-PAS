{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 クリケット\n",
      "1 タイガース\n",
      "2 サッカー\n",
      "3 メッツ\n",
      "4 カブス\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "# Tokenize input\n",
    "text = 'テレビでサッカーの試合を見る。'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "# ['テレビ', 'で', 'サッカー', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 2\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "# ['テレビ', 'で', '[MASK]', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# [571, 12, 4, 5, 608, 11, 2867, 8]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# tensor([[ 571,   12,    4,    5,  608,   11, 2867,    8]])\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0][0, masked_index].topk(5) # 予測結果の上位5件を抽出\n",
    "\n",
    "# Show results\n",
    "for i, index_t in enumerate(predictions.indices):\n",
    "    index = index_t.item()\n",
    "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db4de92f9330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mBertJapaneseTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n",
       "\n",
       "Args:\n",
       "    pretrained_model_name_or_path: either:\n",
       "\n",
       "        - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n",
       "        - a string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n",
       "        - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
       "        - (not applicable to all derived classes, deprecated) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n",
       "\n",
       "    cache_dir: (`optional`) string:\n",
       "        Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n",
       "\n",
       "    force_download: (`optional`) boolean, default False:\n",
       "        Force to (re-)download the vocabulary files and override the cached versions if they exists.\n",
       "\n",
       "    resume_download: (`optional`) boolean, default False:\n",
       "        Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n",
       "\n",
       "    proxies: (`optional`) dict, default None:\n",
       "        A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n",
       "        The proxies are used on each request.\n",
       "\n",
       "    inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n",
       "\n",
       "    kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n",
       "\n",
       "    # Download vocabulary from S3 and cache.\n",
       "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
       "\n",
       "    # Download vocabulary from S3 (user-uploaded) and cache.\n",
       "    tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
       "\n",
       "    # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
       "    tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
       "\n",
       "    # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
       "    tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
       "\n",
       "    # You can link tokens to special vocabulary when instantiating\n",
       "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
       "    # You should be sure '<unk>' is in the vocabulary when doing that.\n",
       "    # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
       "    assert tokenizer.unk_token == '<unk>'\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.8.2/lib/python3.8/site-packages/transformers/tokenization_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BertJapaneseTokenizer.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
